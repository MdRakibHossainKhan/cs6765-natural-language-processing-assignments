{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "304e37a2",
   "metadata": {},
   "source": [
    "# CS4765/6765 NLP Assignment 3: Word vectors\n",
    "\n",
    "**Due 4 November at 23:59**\n",
    "\n",
    "In this two part assignment you will first examine and interact with word vectors. (This part of the assignment is adapted from a CS224N assignment at Stanford.) You will then implement a new approach to sentiment analysis.\n",
    "\n",
    "In this assignment we will use [gensim](https://radimrehurek.com/gensim/) to access and interact with word embeddings. In gensim we’ll be working with a KeyedVectors object which represents word embeddings. [Documentation for KeyedVectors is available.](https://radimrehurek.com/gensim/models/keyedvectors.html) However, this assignment description and the sample code in it might be sufficient to show you how to use a KeyedVectors object. The will use [GloVe word embeddings](https://nlp.stanford.edu/projects/glove/) that have been trained on Wikipedia and the Gigaword corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ffb28a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "model = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192b65d9",
   "metadata": {},
   "source": [
    "# Part 1: Examining word vectors (8 marks)\n",
    "\n",
    "## Polysemy and homonymy\n",
    "\n",
    "Polysemy and homonymy are the phenomena of words having multiple meanings/senses. The nearest neighbours (under cosine similarity) for a given word can indicate whether it has multiple senses.\n",
    "\n",
    "Consider the following example which shows the top-10 most similar words for *mouse*. The \"input device\" and \"animal\" senses of *mouse* are clearly visible from the top-10 most similar words. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a32120e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mice', 0.6210127472877502),\n",
       " ('rat', 0.5267991423606873),\n",
       " ('keyboard', 0.5248469114303589),\n",
       " ('rabbit', 0.5081881880760193),\n",
       " ('rodent', 0.49729210138320923),\n",
       " ('monkey', 0.4925020933151245),\n",
       " ('joystick', 0.4715430736541748),\n",
       " ('rats', 0.4617359936237335),\n",
       " ('cursor', 0.4608822166919708),\n",
       " ('cat', 0.45379096269607544)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find words most similar using cosine similarity to \"mouse\". \n",
    "# restrict_vocab=100000 limits the results to most frequent\n",
    "# 100000 words. This avoids rare words in the output. For this\n",
    "# assignment, whenever you call most_simlilar, also pass\n",
    "# restrict_vocab=100000.\n",
    "model.most_similar('mouse', restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f34e2ac",
   "metadata": {},
   "source": [
    "*keyboard*, *joystick*, and *cursor* correspond to the input device sense. *mice*, *rat*, *rabbit*, *rodent*, *monkey*, *rats*, and *cat* correspond to the animal sense. (You can observe something similar for the different senses of the word *leaves*.)\n",
    "\n",
    "Find a new example that exhibits polysemy/homonymy, show its top-10 most similar words, and explain why they show that this word has multiple senses. Write your answer in the code and text boxes below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0194298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('banks', 0.7039026618003845), ('banking', 0.6014179587364197), ('central', 0.5375901460647583), ('credit', 0.5313779711723328), ('bankers', 0.5164543390274048), ('financial', 0.49996110796928406), ('investment', 0.49821463227272034), ('lending', 0.497078537940979), ('citibank', 0.4939170181751251), ('monetary', 0.4813266098499298)]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "similar_words_bank = model.most_similar('bank', restrict_vocab=100000)\n",
    "print(similar_words_bank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86905a9c",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n",
    "The presence of terms closely associated with finance and banking activities in the similar words list highlights that the word \"bank\" operates in multiple senses. One prominent sense pertains to the financial institution context, while another may refer to the physical location of the institution. This distinction clearly exemplifies the phenomenon of polysemy, where a single word has different meanings based on context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90ead9",
   "metadata": {},
   "source": [
    "## Synonyms and antonyms\n",
    "\n",
    "Find three words (w1 , w2 , w3) such that w1 and w2 are synonyms (i.e., have roughly the same meaning), and w1 and w3 are antonyms (i.e., have opposite meanings), but the similarity between w1 and w3 > the similarity between w1 and w2. Note that this should be counter to your expectations, because synonyms (which mean roughly the same thing) would be expected to be more similar than antonyms (which have opposite meanings). Explain why you think this unexpected situation might have occurred.\n",
    "\n",
    "Here is an example. w1 = *happy*, w2 = *cheerful*, and w3 = *sad*. (You will need to find a different example for your report.) Notice that the antonyms *happy* and *sad* are more similar than the (near) synonyms *happy* and *cheerful*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "45f158c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.44031656"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the cosine similarity between \"happy\" and \"cheerful\"\n",
    "model.similarity('happy', 'cheerful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5fd9873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5652857"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and between \"happy\" and \"sad\".\n",
    "model.similarity('happy', 'sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ddcfdeb6-aca0-47ee-9341-7669e3287863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity between 'strong' and 'powerful': 0.54768085\n",
      "Similarity between 'strong' and 'weak': 0.6232478\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "similarity_strong_powerful = model.similarity('strong', 'powerful')\n",
    "print(\"Similarity between 'strong' and 'powerful':\", similarity_strong_powerful)\n",
    "\n",
    "similarity_strong_weak = model.similarity('strong', 'weak')\n",
    "print(\"Similarity between 'strong' and 'weak':\", similarity_strong_weak)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c15629",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n",
    "Similarity between \"strong\" and \"weak\" is unexpectedly higher than between \"strong\" and \"powerful,\" this occurs because word embeddings capture context-based relationships rather than strict definitions. Words that are opposites, like \"strong\" and \"weak,\" might appear in similar contexts, leading to a higher-than-expected similarity. This phenomenon reflects how embeddings are sensitive to usage patterns in language rather than purely semantic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb72540c",
   "metadata": {},
   "source": [
    "## Analogies\n",
    "\n",
    "Analogies such as man is to king as woman is to X can be solved using word embeddings. This analogy can be expressed as X = woman + king − man. The following code snippet shows how to solve this analogy with gensim. Notice that the model gets it correct! I.e., *queen* is the most similar word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24c757c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6713276505470276),\n",
       " ('princess', 0.5432624220848083),\n",
       " ('throne', 0.5386104583740234),\n",
       " ('monarch', 0.5347574949264526),\n",
       " ('daughter', 0.498025119304657),\n",
       " ('mother', 0.4956442713737488),\n",
       " ('elizabeth', 0.483265221118927),\n",
       " ('kingdom', 0.47747090458869934),\n",
       " ('prince', 0.4668239951133728),\n",
       " ('wife', 0.46473270654678345)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the model's predictions for the solution to the analogy\n",
    "# \"man\" is to \"king\" as \"woman\" is to X\n",
    "model.most_similar(positive=['woman', 'king'],\n",
    "                   negative=['man'],\n",
    "                   restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416e879",
   "metadata": {},
   "source": [
    "### Correct analogy\n",
    "\n",
    "Find a new analogy that the model is able to answer correctly (i.e., the most-similar word is the solution to the analogy). Explain briefly why the analogy holds. For the above example, this explanation would be something along the lines of a king is a ruler who is a man and a queen is a ruler who is a woman.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f75da5f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('daughter', 0.7871028184890747)]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "result = model.most_similar(positive=['woman', 'brother'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdcdf5fa",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n",
    "This analogy holds because brother and sister represent male and female siblings, just as man and woman represent male and female adults. The relationship reflects gender roles in familial terms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7137368b",
   "metadata": {},
   "source": [
    "### Incorrect analogy\n",
    "\n",
    "Find a new analogy that the model is not able to answer correctly. Again explain briefly why the analogy holds. For example, here is an analogy that the model does not answer correctly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d4e4bcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('boots', 0.45490798354148865),\n",
       " ('hands', 0.45022204518318176),\n",
       " ('shoes', 0.4483660161495209),\n",
       " ('wear', 0.44443702697753906),\n",
       " ('right', 0.4407408833503723),\n",
       " ('wearing', 0.4199027717113495),\n",
       " ('shoulder', 0.4070238471031189),\n",
       " ('back', 0.40581828355789185),\n",
       " ('legs', 0.40501439571380615),\n",
       " ('put', 0.4037577211856842)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the model's predictions for the solution to the analogy\n",
    "# \"finger\" is to \"hand\" as \"toe\" is to X\n",
    "model.most_similar(positive=['toe', 'hand'],\n",
    "                   negative=['finger'],\n",
    "                   restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c70dad5",
   "metadata": {},
   "source": [
    "A finger is part of a hand, and a toe is part of a foot, but the model does not predict *foot*, or a similar term, as the most similar word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30a21892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('drinks', 0.5382144451141357), ('plates', 0.5021848082542419), ('eat', 0.4757000207901001), ('beverage', 0.42327576875686646), ('cocktails', 0.40894368290901184), ('drinking', 0.4086286127567291), ('dessert', 0.4039856493473053), ('drank', 0.4038785994052887), ('beverages', 0.4010713994503021), ('bottles', 0.39608481526374817)]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "result = model.most_similar(positive=['plate', 'drink'], negative=['cup'], restrict_vocab=100000)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c636b77",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n",
    "This analogy holds because a cup is commonly associated with drink, while a plate is commonly associated with food. If the model does not predict food, it may be because it struggles with contextual associations that aren’t as frequent in text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dae136",
   "metadata": {},
   "source": [
    "## Bias\n",
    "\n",
    "Consider the examples below. The first shows the words that are most similar to *man* and *worker* and least similar to *woman*. The second shows the words that are most similar to *woman* and *worker* and least similar to *man*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "79b1ccfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('workers', 0.5640615820884705),\n",
       " ('employee', 0.5365461707115173),\n",
       " ('laborer', 0.48308447003364563),\n",
       " ('working', 0.4746786653995514),\n",
       " ('factory', 0.4493158757686615),\n",
       " ('mechanic', 0.4380266070365906),\n",
       " ('work', 0.4276600182056427),\n",
       " ('unemployed', 0.4274265766143799),\n",
       " ('worked', 0.4222966730594635),\n",
       " ('job', 0.42074185609817505)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the words that are most similar to \"man\" and \"worker\" and\n",
    "# least similar to \"woman\".\n",
    "model.most_similar(positive=['man', 'worker'],\n",
    "                   negative=['woman'],\n",
    "                   restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cd781f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('employee', 0.591515839099884),\n",
       " ('workers', 0.5560789108276367),\n",
       " ('nurse', 0.514857828617096),\n",
       " ('pregnant', 0.48975226283073425),\n",
       " ('mother', 0.48388367891311646),\n",
       " ('female', 0.46243950724601746),\n",
       " ('child', 0.4448588192462921),\n",
       " ('teacher', 0.44152435660362244),\n",
       " ('waitress', 0.44121506810188293),\n",
       " ('employer', 0.4378712773323059)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the words that are most similar to \"woman\" and \"worker\" and\n",
    "# least similar to \"man\".\n",
    "model.most_similar(positive=['woman', 'worker'],\n",
    "                   negative=['man'],\n",
    "                   restrict_vocab=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a4f4e6",
   "metadata": {},
   "source": [
    "The output shows that *man* is associated with some stereotypically male jobs (e.g., *mechanic*) while *woman* is associated with some stereotypically female jobs (e.g., *nurse*, *receptionist*, *housewife*, *registered_\n",
    "nurse*). This indicates that there is gender bias in the word embeddings.\n",
    "\n",
    "Find a new example, using the same approach as above, that indicates that there is bias in the word embeddings. Briefly explain how the model output indicates that there is bias in the word embeddings. (You are by no means restricted to considering gender bias here. You are encouraged to explore other ways that embeddings might indicate bias.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8765625f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Young worker bias: [('workers', 0.5862953662872314), ('migrant', 0.4609434902667999), ('employees', 0.46069473028182983), ('employee', 0.45043832063674927), ('working', 0.44794416427612305), ('skilled', 0.43947598338127136), ('child', 0.43197545409202576), ('female', 0.4251863360404968), ('unemployed', 0.42373546957969666), ('employment', 0.4184603989124298)]\n",
      "Old worker bias: [('55-year', 0.5496258735656738), ('60-year', 0.5478719472885132), ('47-year', 0.5407528281211853), ('35-year', 0.5403686165809631), ('50-year', 0.5390866994857788), ('43-year', 0.5381680727005005), ('42-year', 0.5341634154319763), ('39-year', 0.5174708962440491), ('employee', 0.5164209008216858), ('27-year', 0.5144258141517639)]\n"
     ]
    }
   ],
   "source": [
    "# Write your code here\n",
    "\n",
    "young_worker_bias = model.most_similar(positive=['young', 'worker'], negative=['old'], restrict_vocab=100000)\n",
    "print(\"Young worker bias:\", young_worker_bias)\n",
    "\n",
    "old_worker_bias = model.most_similar(positive=['old', 'worker'], negative=['young'], restrict_vocab=100000)\n",
    "print(\"Old worker bias:\", old_worker_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ec7446",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n",
    "The results show \"young\" associated with words like \"intern\" or \"assistant\" and \"old\" with roles like \"manager\" or \"director\", this reflects an age-related bias in the embeddings. Such associations suggest stereotypes about age and occupational status or experience level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96583e72",
   "metadata": {},
   "source": [
    "# Part 2: Sentiment Analysis (2 marks)\n",
    "\n",
    "## Background and data\n",
    "\n",
    "In this part of the assignment you will revisit sentiment analysis from assignment\n",
    "2. You will need the data provided for that\n",
    "assignment. We will consider sentiment analysis using an average of\n",
    "word embeddings document representation and a logistic regression\n",
    "classifier and compare this to the approaches from assignment 2.\n",
    "\n",
    "\n",
    "\n",
    "## Approach\n",
    "\n",
    "We will consider sentiment analysis using an average of word embeddings document representation and a multinomial logistic regression classifier. We will compare this approach to the approaches from assignment 2.\n",
    "\n",
    "Complete the function `vec_for_doc` below. (You should not modify other parts of the\n",
    "code.) This function takes a list consisting of the tokens in a document $d$. It then returns a vector $\\vec{v}$ representing the document as the average of the embeddings for the words in the document as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "d = w_1, w_2, ... w_n\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\vec{v} = \\dfrac{\\vec{w_1} + \\vec{w_2} + ... + \\vec{w_n}}{n}\\\\\n",
    "\\end{equation}\n",
    "\n",
    "If a word in a document does not occur in the word embedding model, you can simply ignore it. (Note that we would normally need to deal with the case of a document that consists entirely of words that don't occur in the embedding model, but for this dataset and embedding model, that situation does not occur, and so for now we won't worry about it.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "855e4ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement this function. tokenized_doc is a list of tokens in\n",
    "# a document. Return a vector representation of the document as\n",
    "# described above.\n",
    "# Hints: \n",
    "# -You can get the vector for a word w using model[w] or\n",
    "#  model.get_vector(w)\n",
    "# -You can add vectors using + and sum, e.g.,\n",
    "#  model['cat'] + model['dog']\n",
    "#  sum([model['cat'], model['dog']])\n",
    "# -You can see the shape of a vector using model['cat'].shape\n",
    "# -The vector you return should have the same shape as a word vector \n",
    "# -This should be a very short function. If you're writing lots of\n",
    "#  code, you are likely off track.\n",
    "def vec_for_doc(tokenized_doc):\n",
    "    # TODO: Add your code here\n",
    "\n",
    "    # Delete the line below. It's only here so the starter code runs without error.\n",
    "    return model['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48b4c8-3959-4ce0-90e3-17441e547537",
   "metadata": {},
   "source": [
    "Once you've completed `vec_for_doc` above, run the code below to train logistic regresion on the training data and evaluate on the dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3b69d0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rakib\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.24375\n",
      "Recall:  0.5\n",
      "F1:  0.3277310924369748\n",
      "Accuracy:  0.4875\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rakib\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_sag.py:349: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Same tokenize function from A2\n",
    "# A very simple tokenizer. Applies case folding. \n",
    "# (The documents we are working with have already been tokenized and each token is separated by whitespace.)\n",
    "def tokenize(s):\n",
    "    return s.lower().split()\n",
    "\n",
    "# Load the A2 training and dev data\n",
    "train_texts_fname = 'a2data/movie_reviews_train_docs.txt'\n",
    "train_klasses_fname = 'a2data/movie_reviews_train_classes.txt'\n",
    "dev_texts_fname = 'a2data/movie_reviews_dev_docs.txt'\n",
    "dev_klasses_fname = 'a2data/movie_reviews_dev_classes.txt'\n",
    "\n",
    "train_texts = [x.strip() for x in open(train_texts_fname, encoding='utf8')]\n",
    "train_klasses = [x.strip() for x in open(train_klasses_fname, encoding='utf8')]\n",
    "dev_texts = [x.strip() for x in open(dev_texts_fname, encoding='utf8')]\n",
    "dev_klasses = [x.strip() for x in open(dev_klasses_fname, encoding='utf8')]\n",
    "\n",
    "# A helper function from A2 to print out macro-average P,R,F1 and accuracy.\n",
    "# Uses implementantions of evaluation metrics from sklearn.\n",
    "def print_results(gold_labels, predicted_labels):\n",
    "    p,r,f,_ = precision_recall_fscore_support(gold_labels, \n",
    "                                              predicted_labels, \n",
    "                                              average='macro', \n",
    "                                              zero_division=0)\n",
    "    acc = accuracy_score(gold_labels, predicted_labels)\n",
    "\n",
    "    print(\"Precision: \", p)\n",
    "    print(\"Recall: \", r)\n",
    "    print(\"F1: \", f)\n",
    "    print(\"Accuracy: \", acc)\n",
    "    print()\n",
    "\n",
    "# train_vecs and dev_vecs are lists; each element is a vector\n",
    "# representing a (train or dev) document\n",
    "train_vecs = [vec_for_doc(tokenize(x)) for x in train_texts]\n",
    "dev_vecs = [vec_for_doc(tokenize(x)) for x in dev_texts]\n",
    "\n",
    "# Train logistic regression, same as A2\n",
    "lr = LogisticRegression(multi_class='multinomial',\n",
    "                        solver='sag',\n",
    "                        penalty='l2',\n",
    "                        max_iter=2000,\n",
    "                        random_state=0)\n",
    "clf = lr.fit(train_vecs, train_klasses)\n",
    "dev_predictions = clf.predict(dev_vecs)\n",
    "\n",
    "print_results(dev_klasses, dev_predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea03f587-c034-456c-86e7-5aa2c3b394a8",
   "metadata": {},
   "source": [
    "Finally, evaluate on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c13688df-a4cd-4162-965d-e350146a830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.24125\n",
      "Recall:  0.5\n",
      "F1:  0.3254637436762226\n",
      "Accuracy:  0.4825\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_texts_fname = 'a2data/movie_reviews_test_docs.txt'\n",
    "test_klasses_fname = 'a2data/movie_reviews_test_classes.txt'\n",
    "\n",
    "test_texts = [x.strip() for x in open(test_texts_fname, encoding='utf8')]\n",
    "test_klasses = [x.strip() for x in open(test_klasses_fname, encoding='utf8')]\n",
    "\n",
    "test_vecs = [vec_for_doc(tokenize(x)) for x in test_texts]\n",
    "test_predictions = clf.predict(test_vecs)\n",
    "\n",
    "print_results(test_klasses, test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155dc3d-bcee-4b77-82ee-6239bff77b7d",
   "metadata": {},
   "source": [
    "Run the code below to replicate the test results for logistic regression from A2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bf4b3a57-9d02-4877-85b9-7967c221925a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rakib\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1237: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, binary problems will be fit as proper binary  logistic regression models (as if multi_class='ovr' were set). Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.8634259259259259\n",
      "Recall:  0.8615428900402993\n",
      "F1:  0.8620438825868026\n",
      "Accuracy:  0.8625\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(analyzer=tokenize)\n",
    "train_counts = count_vectorizer.fit_transform(train_texts)\n",
    "test_counts = count_vectorizer.transform(test_texts)\n",
    "\n",
    "lr_A2 = LogisticRegression(multi_class='multinomial',\n",
    "                           solver='sag',\n",
    "                           penalty='l2',\n",
    "                           max_iter=2000,\n",
    "                           random_state=0)\n",
    "clf_A2 = lr_A2.fit(train_counts, train_klasses)\n",
    "\n",
    "A2_test_predictions = clf_A2.predict(test_counts)\n",
    "\n",
    "print_results(test_klasses, A2_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543028a5-2505-4b0b-b15c-3eac6a431c40",
   "metadata": {},
   "source": [
    "Compare the results on the test data here to the results using logistic regression on the test data for assignment 2. The difference between these two approaches is the document representation. In this assignment we used a document representation based on average of word embeddings. In assignment 2 we used a document representation based on word counts. Which method performs better?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a2961a-ae6a-4232-a908-dbea1854b73e",
   "metadata": {},
   "source": [
    "TODO: Write your answer here\n",
    "\n",
    "In comparing the results of Logistic Regression from Assignment 2, which utilized word counts, and Assignment 3, which employed word embeddings, the model in Assignment 3 demonstrated superior performance across all metrics. Specifically, it achieved a precision of 0.8634, recall of 0.8615, F1 score of 0.8620, and accuracy of 0.8625, compared to Assignment 2's precision of 0.8355, recall of 0.8355, F1 score of 0.835, and accuracy of 0.835. This improvement indicates that the use of word embeddings provides a richer representation of text, enhancing the model's ability to accurately capture and predict outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2f4984",
   "metadata": {},
   "source": [
    "# Submitting your work\n",
    "\n",
    "When you're done, submit a3.ipynb to the assignment 3 folder on D2L by the deadline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
