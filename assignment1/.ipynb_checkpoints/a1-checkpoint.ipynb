{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e61b175-4769-4097-bfbc-fc91835a48fe",
   "metadata": {},
   "source": [
    "# CS4765/6765 Assignment 1: Tokenization and Counting Words (from Russian Trolls)\n",
    "\n",
    "This (relatively short) assignment will give you experience working\n",
    "with Python, corpora, regular expressions, tokenization, and some NLP terminology related to words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee761998-c25d-43f4-bd69-481b907eed56",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "**In this assignment you will be working with real tweets. Some\n",
    "  of the tweets contain content that you might find offensive (e.g.,\n",
    "  expletives, racist and homophobic remarks). Despite this offensive\n",
    "  content, tweets can still be very valuable data, and building NLP\n",
    "  systems that can operate over them is important. That is why we are\n",
    "  working with this potentially-offensive data in this assignment.**\n",
    "\n",
    "I've provided you with the following file for this assignment:\n",
    "\n",
    "- `russian-troll-tweets-en.txt.gz` A collection of roughly\n",
    "  $750k$ English tweets sent by Russian trolls, mostly from\n",
    "  2015-2017. Each line of this file is a single tweet. It is UTF-8 encoded. (If\n",
    "  you want to take a look at the contents of a file without\n",
    "  uncompressing it, `zcat` is helpful. On OSX I use `gzcat`.) You can read more about the project that collected these tweets here:\n",
    "https://fivethirtyeight.com/features/why-were-sharing-3-million-russian-troll-tweets/\n",
    "\n",
    "## Implementation\n",
    "\n",
    "You will implement tokenization and counting in this assignment. You must not use NLTK or any other NLP toolkits. You should not import any modules that this notebook does not already import for you. Your code must be able to run on the NLP VM on the lab machines using Python 3.9.\n",
    "\n",
    "It is important that your code be reasonably efficient in this assignment. In particular, your code must not make multiple passes over the corpus, and must not read the entire corpus into memory at once. Corpora often contain billions of tokens. The corpus we are working with in this assignment is relatively small. (You will determine the size in tokens as part of the assignment). These efficiency issues become particularly important when working with larger corpora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf1fc20-9144-4732-8a27-97d11149dbdd",
   "metadata": {},
   "source": [
    "## Tokenization (3 marks)\n",
    "\n",
    "You will first write a function, `tokenize`, to tokenize tweets. This function takes a line/tweet as input and returns a list where each element is a token from the line/tweet. To tokenize tweets, use regular expressions. The following describes the tokenization:\n",
    "\n",
    "- Any sequence of alphanumeric characters, underscores, hyphens, or apostrophes, that optionally begins with # or @, is a token.\n",
    "- Any sequence of other non-whitespace characters is a token;\n",
    "- Any sequence of whitespace characters is a token boundary. (Whitespace does not appear as tokens in the output.)\n",
    "\n",
    "**Hint:** I used the re.split function in my solution; you might find this helpful too.\n",
    "\n",
    "The test cases provided below help to show the expected behaviour of the tokenizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32822098-f617-4134-8e7d-a1b99cf2690c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(l):\n",
    "    # A very simple whitespace-based tokenizer. You will need to\n",
    "    # improve this function for your assignment. You will probably\n",
    "    # need some regular expressions, so I've already imported the re\n",
    "    # module for you :-)\n",
    "\n",
    "    tokens = re.findall(r\"[@#]?\\w+(?:[-']\\w+)*|[:;][()D]|[!?]+|[^\\s\\w]\", l)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb4add5-1bba-4d83-b9a1-f38230647fec",
   "metadata": {},
   "source": [
    "Test `tokenize` on three (carefully chosen and lightly-edited) tweets (from a different Twitter corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475e1d86-e47f-40e9-b7a3-610bc6d627fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_at_hashtag_smiley (__main__.TestTokenization.test_at_hashtag_smiley) ... ok\n",
      "test_clitic (__main__.TestTokenization.test_clitic) ... ok\n",
      "test_simple (__main__.TestTokenization.test_simple) ... ok\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 3 tests in 0.006s\n",
      "\n",
      "OK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x22535aa5f40>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestTokenization(unittest.TestCase):\n",
    "    def test_simple(self):\n",
    "        self.assertEqual(tokenize('''I just ate a whole bag of chips, help!!!'''),\n",
    "                        ['I',\n",
    "                         'just', \n",
    "                         'ate', \n",
    "                         'a', \n",
    "                         'whole', \n",
    "                         'bag', \n",
    "                         'of', \n",
    "                         'chips', \n",
    "                         ',', \n",
    "                         'help', \n",
    "                         '!!!'])\n",
    "\n",
    "    def test_clitic(self):\n",
    "        self.assertEqual(tokenize('''Now I'm tired?'''),\n",
    "                         ['Now', \n",
    "                          \"I'm\", \n",
    "                          'tired', \n",
    "                          '?'])\n",
    "\n",
    "    def test_at_hashtag_smiley(self):\n",
    "        self.assertEqual(tokenize('''@USER please bring back @USER, pleaseeee :( #AlwaysInOurHearts'''),\n",
    "                         ['@USER',\n",
    "                          'please',\n",
    "                          'bring',\n",
    "                          'back',\n",
    "                          '@USER',\n",
    "                          ',',\n",
    "                          'pleaseeee',\n",
    "                          ':(',\n",
    "                          '#AlwaysInOurHearts'])\n",
    "            \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924ebfaa-9da3-4bde-851a-e9837d0a981c",
   "metadata": {},
   "source": [
    "Tokenize the corpus and write the output to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d88d13af-e88d-419c-a83e-45fc40d6ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "def tokenize_file(in_fname, out_fname):\n",
    "    # Apply tokenize to each line in in_fname and write output\n",
    "    # one-token-per-line to out_fname. This one-token-per-line\n",
    "    # format for a corpus is also referred to as \"vertical\" format.\n",
    "    with gzip.open(in_fname, mode='rt', encoding='utf-8') as infile:\n",
    "        with open(out_fname, mode='w', encoding='utf-8') as outfile:\n",
    "            for line in infile:\n",
    "                # Tokenize each sentence/line\n",
    "                tokens = tokenize(line)\n",
    "                # Write each token on a separate line, with a blank line between\n",
    "                # sentences\n",
    "                for t in tokens:\n",
    "                    print(t, file=outfile)\n",
    "                print(file=outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "219e4d8f-8c7c-44b2-a300-17534eb944f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_file('a1data/russian-troll-tweets-en.txt.gz', 'russian-troll-tweets-en.tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e88a44d-7520-4c94-be5c-28db77d0a3cb",
   "metadata": {},
   "source": [
    "## Counting (2 marks)\n",
    "\n",
    "Now write a function, `count_tokens`, that counts how many times each type occurs in a\n",
    "corpus. In doing this counting, apply case folding (i.e., convert\n",
    "everything to lower case). Your function takes the filename for a file such as `russian-troll-tweets-en.tokens` as input (i.e., a file in one-token-per-line format) and returns dictionary in which the keys are types (ignoring case) and the value for each key is its count in the corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ed96e6a-0bf4-46e2-bb7c-853bdc735954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(fname):\n",
    "    counts = {}\n",
    "\n",
    "    # Write your code here\n",
    "\n",
    "    with open(fname, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            token = line.strip().lower()\n",
    "            \n",
    "            if token in counts:\n",
    "                counts[token] += 1\n",
    "            else:\n",
    "                counts[token] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845547a4-cd6c-4be7-879f-a320f06e2071",
   "metadata": {},
   "source": [
    "Apply the counting to the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a3bc4290-afe8-4308-9956-8ba2a4fbe030",
   "metadata": {},
   "outputs": [],
   "source": [
    "russian_troll_counts = count_tokens('russian-troll-tweets-en.tokens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2c2d09-bafe-432d-bbda-50a9075dfd02",
   "metadata": {},
   "source": [
    "Here are some test cases. These pass using my sample solution to the tokenizer and counting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d942816-53dc-4a6e-9a6b-a418aedeef08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test_3exclamationmarks (__main__.TestCounting.test_3exclamationmarks) ... FAIL\n",
      "test_cat (__main__.TestCounting.test_cat) ... FAIL\n",
      "test_duck (__main__.TestCounting.test_duck) ... FAIL\n",
      "test_smiley (__main__.TestCounting.test_smiley) ... FAIL\n",
      "test_test (__main__.TestCounting.test_test) ... FAIL\n",
      "test_at_hashtag_smiley (__main__.TestTokenization.test_at_hashtag_smiley) ... ok\n",
      "test_clitic (__main__.TestTokenization.test_clitic) ... ok\n",
      "test_simple (__main__.TestTokenization.test_simple) ... ok\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_3exclamationmarks (__main__.TestCounting.test_3exclamationmarks)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rakib\\AppData\\Local\\Temp\\ipykernel_604\\516314098.py\", line 13, in test_3exclamationmarks\n",
      "    self.assertEqual(russian_troll_counts['!!!'], 3213)\n",
      "AssertionError: 3412 != 3213\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_cat (__main__.TestCounting.test_cat)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rakib\\AppData\\Local\\Temp\\ipykernel_604\\516314098.py\", line 7, in test_cat\n",
      "    self.assertEqual(russian_troll_counts['cat'], 932)\n",
      "AssertionError: 940 != 932\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_duck (__main__.TestCounting.test_duck)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rakib\\AppData\\Local\\Temp\\ipykernel_604\\516314098.py\", line 5, in test_duck\n",
      "    self.assertEqual(russian_troll_counts['duck'], 294)\n",
      "AssertionError: 322 != 294\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_smiley (__main__.TestCounting.test_smiley)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rakib\\AppData\\Local\\Temp\\ipykernel_604\\516314098.py\", line 11, in test_smiley\n",
      "    self.assertEqual(russian_troll_counts[':)'], 779)\n",
      "AssertionError: 870 != 779\n",
      "\n",
      "======================================================================\n",
      "FAIL: test_test (__main__.TestCounting.test_test)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Rakib\\AppData\\Local\\Temp\\ipykernel_604\\516314098.py\", line 9, in test_test\n",
      "    self.assertEqual(russian_troll_counts['test'], 1591)\n",
      "AssertionError: 1601 != 1591\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.021s\n",
      "\n",
      "FAILED (failures=5)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<unittest.main.TestProgram at 0x22538107e30>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unittest\n",
    "\n",
    "class TestCounting(unittest.TestCase):\n",
    "    def test_duck(self):\n",
    "        self.assertEqual(russian_troll_counts['duck'], 294)\n",
    "    def test_cat(self):\n",
    "        self.assertEqual(russian_troll_counts['cat'], 932)\n",
    "    def test_test(self):\n",
    "        self.assertEqual(russian_troll_counts['test'], 1591)\n",
    "    def test_smiley(self):\n",
    "        self.assertEqual(russian_troll_counts[':)'], 779)\n",
    "    def test_3exclamationmarks(self):\n",
    "        self.assertEqual(russian_troll_counts['!!!'], 3213)\n",
    "        \n",
    "unittest.main(argv=[''], verbosity=2, exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e4277-80d7-4910-ad9a-9b0ac78f30af",
   "metadata": {},
   "source": [
    "## Questions (5 marks)\n",
    "\n",
    "Answer the following questions. For some questions, you will need to write some code to get the answer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912dac05-1363-422f-85eb-36012ad1eee9",
   "metadata": {},
   "source": [
    "1. How many types are in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a051e1f5-4caa-4b3c-94b1-56e65aac831b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216287 types\n"
     ]
    }
   ],
   "source": [
    "# Write code to help answer this question here\n",
    "\n",
    "corpus_type_count = len(russian_troll_counts)\n",
    "print(f\"{corpus_type_count} types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b7db83-2bb5-4cec-9088-97076fc5a092",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5b71f-816c-4db1-b217-c43ba40ae14d",
   "metadata": {},
   "source": [
    "2. How many tokens are in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6b2ab432-79d1-491b-aa3e-8e20036cda75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11856883 tokens\n"
     ]
    }
   ],
   "source": [
    "# Write code to help answer this question here\n",
    "\n",
    "russian_troll_counts = count_tokens('russian-troll-tweets-en.tokens')\n",
    "total_tokens = sum(russian_troll_counts.values())\n",
    "print(f\"{total_tokens} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40f1a61-30e0-4590-bad1-ca5eeb630ede",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a457474-ef78-440a-8fe9-4304a91f2290",
   "metadata": {},
   "source": [
    "3. How many hapax legomena (types that occur once) are in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f7e8612-31a9-408d-a27f-31b0049ca49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110067 hapax legomena\n"
     ]
    }
   ],
   "source": [
    "# Write code to help answer this question here\n",
    "\n",
    "hapax_legomena = [token for token, count in russian_troll_counts.items() if count == 1]\n",
    "hapax_legomena_num = len(hapax_legomena)\n",
    "print(f\"{hapax_legomena_num} hapax legomena\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645fe45c-5278-433d-b100-1b69f5ec455b",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fea08b-4ed9-428c-a3de-78fe2efad54e",
   "metadata": {},
   "source": [
    "4. What are the 10 most frequent types in the corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e51d7bb7-4bcc-4c48-a93c-297d2bff8be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'' Occurs 759346 Times\n",
      "'.' Occurs 477367 Times\n",
      "',' Occurs 233837 Times\n",
      "'to' Occurs 230468 Times\n",
      "'the' Occurs 224984 Times\n",
      "''' Occurs 186804 Times\n",
      "'in' Occurs 159822 Times\n",
      "'a' Occurs 137100 Times\n",
      "'of' Occurs 131660 Times\n",
      "':' Occurs 125107 Times\n"
     ]
    }
   ],
   "source": [
    "# Write code to help answer the question here\n",
    "\n",
    "most_frequent_types = sorted(russian_troll_counts.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for token, count in most_frequent_types:\n",
    "    print(f\"'{token}' Occurs {count} Times\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079c93e-0362-44a0-9e25-907d5cebc238",
   "metadata": {},
   "source": [
    "Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686398f-cc06-40e9-8aec-e2a658ff7623",
   "metadata": {},
   "source": [
    "5. I carried out some analysis on a corpus consisting of a random sample of ~500$k$ English tweets from a similar time period as the collection of Russian troll tweets that you have been working with in this assignment. This is a sample of all English tweets, i.e., it has not been carefully constructed to represent the language of Russian trolls, or any other group. I used the same approach to tokenization and counting as above. Here are the top-10 types for this corpus of English tweets:\n",
    "\n",
    "    * .\n",
    "    * i\n",
    "    * the\n",
    "    * you\n",
    "    * to\n",
    "    * ,\n",
    "    * a\n",
    "    * and\n",
    "    * my\n",
    "    * me\n",
    "  \n",
    "Compare the top-10 most frequent types for this corpus to the top-10 most frequent types for the corpus of Russian troll tweets from Question 4. Based on this analysis, how is the language of Russian trolls on Twitter different from that of more-general Twitter users?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b549d0df-5462-4849-9eaf-f06ceb2cc45f",
   "metadata": {},
   "source": [
    "Write your answer here\n",
    "\n",
    "Russian language trolls differs from general Twitter users in many ways. For example,\n",
    "\n",
    "Engagement: English tweets focus more on personal interaction, whereas Rusian troll tweets focus less on personal interaction.\n",
    "\n",
    "Punctuation: For a more authoritative tone, troll tweets frequently use punctuation such as quotes, periods etc.\n",
    "\n",
    "Function vs. Content Words: English tweets focus on function words, while troll tweets use more punctuation, showing a broadcast style.\n",
    "\n",
    "Overall, Russian trolls appear to aim for attention and controversy, while general users engage more casually."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068463b8-23d7-4399-a009-ab25ce4f34b2",
   "metadata": {},
   "source": [
    "## What to submit\n",
    "\n",
    "When you're done, submit a1.ipynb to the assignment 1 folder on D2L.\n",
    "\n",
    "## A final note... \n",
    "\n",
    "In this assignment you’re working with real data. You might encounter problems or quirks with file formats, character encodings, etc. If you encounter such issues, please post about it on the bulletin board on D2L.\n",
    "\n",
    "Have fun, and good luck!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
